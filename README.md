# ğŸ›’ E-commerce Analytics Pipeline

A modern data pipeline that ingests raw data from Amazon S3, transforms it in Snowflake using dbt, and orchestrates workflows with Apache Airflow.  
*(Data journey: ğŸŒ©ï¸ S3 â†’ â„ï¸ Snowflake â†’ ğŸ”§ dbt â†’ âœˆï¸ Airflow)*

## ğŸ—ºï¸ Project Overview

This repository contains an end-to-end data pipeline that:  
1. **ğŸ“¥ Ingests raw data** from S3 buckets into Snowflake  
2. **ğŸ”„ Transforms data** using dbt to create star schema models  
3. **ğŸ›ï¸ Orchestrates workflows** with Airflow DAGs  
4. **ğŸ“Š Generates analytics-ready tables** for business intelligence  

## ğŸš€ Key Features

- **â˜ï¸ Cloud Storage**: Raw data stored in AWS S3 buckets  
- **â„ï¸ Snowflake Integration**: Automated data loading into Snowflake  
- **ğŸ”§ dbt Transformations**:  
  - Staging layer for raw data  
  - Star schema models for analytics  
  - Fact & dimension tables  
- **âœˆï¸ Airflow Orchestration**:  
  - Scheduled pipeline runs  
  - Dependency management  
  - Error handling & retries  

---

## ğŸ› ï¸ Technology Stack

| Component       | Technology                 |
|-----------------|----------------------------|
| **â˜ï¸ Cloud Storage** | AWS S3                   |
| **â„ï¸ Data Warehouse**| Snowflake                |
| **ğŸ”§ Transformation**| dbt (Data Build Tool)    |
| **âœˆï¸ Orchestration** | Apache Airflow           |
| **ğŸ™ Version Control**| GitHub                  |

---

## ğŸ“‚ Project Structure

```
ecommerce-analytics/
â”œâ”€â”€ ğŸ“ dbt/                  
â”‚   â”œâ”€â”€ ğŸ“ models/           
â”‚   â”‚   â”œâ”€â”€ ğŸ“ staging/      # Raw source tables
â”‚   â”‚   â”œâ”€â”€ ğŸ“ marts/        # Business-friendly datasets
â”‚   â”‚   â””â”€â”€ ğŸ“ star_schema/  # Fact & dimension tables
â”‚   â”œâ”€â”€ ğŸ“ macros/           # Reusable SQL components
â”‚   â”œâ”€â”€ ğŸ“ snapshots/        # (Future) Slowly Changing Dimensions
â”‚   â”œâ”€â”€ ğŸ“„ dbt_project.yml   # dbt configuration
â”‚   â””â”€â”€ ğŸ“„ packages.yml      # dbt dependencies
â”œâ”€â”€ ğŸ“ airflow/              
â”‚   â””â”€â”€ ğŸ“ dags/             
â”‚       â””â”€â”€ ğŸ“„ pipeline.py   # Airflow workflow definitions
â”œâ”€â”€ ğŸ“„ .gitignore            # Excluded files/folders
â””â”€â”€ ğŸ“„ README.md             # This documentation

---

## ğŸ Getting Started

### âš™ï¸ Prerequisites

- âœ… AWS account with S3 buckets
- âœ… Snowflake account
- âœ… Python 3.8+
- ğŸ”§ [dbt Cloud or CLI](https://www.getdbt.com/)
- âœˆï¸ [Apache Airflow](https://airflow.apache.org/)

### ğŸ”Œ Installation

1. **Clone repository**:
   ```bash
   git clone https://github.com/your-username/ecommerce-analytics.git
   cd ecommerce-analytics
   ```

2. **Install Python dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Configure Snowflake connection**  
   Create `dbt/profiles.yml`:
   ```yaml
   ecommerce:
     target: dev
     outputs:
       dev:
         type: snowflake
         account: <your-account>
         user: <your-username>
         password: <your-password>
         role: <your-role>
         database: <your-database>
         warehouse: <your-warehouse>
         schema: <your-schema>
   ```

4. **Set up Airflow connections**  
   Configure in Airflow UI:
   - â„ï¸ Snowflake connection  

---

## ğŸ•¹ï¸ Usage

### Running dbt Models
```bash
# Run full pipeline ğŸš€
dbt run --profiles-dir dbt/

# Run specific models ğŸ¯  
dbt run --models marts.sales --profiles-dir dbt/
```

### Triggering Airflow DAG
1. Start Airflow webserver:
   ```bash
   airflow webserver --port 8080
   ```

2. Start Airflow scheduler:
   ```bash
   airflow scheduler
   ```

3. Access Airflow UI at `localhost:8080`  
4. Enable and trigger `ecommerce_pipeline` DAG ğŸš¦  

---

## ğŸ”„ Pipeline Details

### Data Flow
1. **ğŸ“¥ Ingestion**: CSV/JSON files from S3 â†’ Snowflake tables  
2. **ğŸ§¹ Staging**: Raw data validation & basic cleaning  
3. **ğŸ”€ Transformation**:  
   - Create dimension tables (Clients, Products, Time)  
   - Build fact tables (Sales, Orders)  
4. **ğŸ›ï¸ Orchestration**: Airflow manages dependencies between tasks  

### Key Models
| Model              | Type     | Description                     |
|--------------------|----------|---------------------------------|
| `stg_orders`       | ğŸ§± Staging  | Raw order data from S3          |
| `dim_clients`      | ğŸ“ Dimension| Client information              |
| `fact_sales`       | ğŸ’° Fact     | Sales transactions with metrics|

---

## ğŸ¤ Contributing

1. ğŸ´ Fork the repository  
2. ğŸŒ± Create feature branch (`git checkout -b feature/improvement`)  
3. ğŸ’¾ Commit changes (`git commit -m 'Add new feature'`)  
4. ğŸ“¤ Push to branch (`git push origin feature/improvement`)  
5. ğŸ”€ Open Pull Request  

---

âš ï¸ **Note**: This project requires proper configuration of AWS and Snowflake credentials before use.  
ğŸ”’ **Never commit sensitive information to version control!**

---

Made with â¤ï¸ by Guillermo Fiallo Montero
